# Add the local library to the library paths
.libPaths(c("../R_libs", .libPaths()))

# Load necessary libraries
library(data.table)

# --- Define Submission File Paths ---
# These are the files generated by your best performing models
xgb_file <- "submissions/submission_xgb_tuned.csv"
nn_file <- "submissions/submission_nn_tuned.csv"
ensemble_file <- "submissions/submission_ensemble.csv"

# --- Check if submission files exist ---
if (!file.exists(xgb_file)) {
  stop("XGBoost submission file not found. Please run 'scripts/model_xgb_tuned.R' first.")
}
if (!file.exists(nn_file)) {
  stop("Neural Network submission file not found. Please run 'scripts/model_nn_tuned.R' first.")
}

cat("Reading submission files...\n")
xgb_sub <- fread(xgb_file)
nn_sub <- fread(nn_file)

# --- Create the Ensemble ---
# Set column names for merging
setnames(xgb_sub, "y_passXtremeDurability", "xgb_pred")
setnames(nn_sub, "y_passXtremeDurability", "nn_pred")

# Merge the predictions by ID to ensure they are aligned
cat("Merging predictions...\n")
ensemble <- merge(xgb_sub, nn_sub, by = "ID")

# Define the weights for the ensemble. A 50/50 split is a good starting point.
# You can experiment with these weights (e.g., 0.6 and 0.4) to see if you can improve your score.
xgb_weight <- 0.5
nn_weight <- 0.5

cat(paste("Creating ensemble with weights: XGB =", xgb_weight, ", NN =", nn_weight, "\n"))
ensemble[, y_passXtremeDurability := (xgb_pred * xgb_weight) + (nn_pred * nn_weight)]

# --- Save the Final Submission File ---
final_submission <- ensemble[, .(ID, y_passXtremeDurability)]
write.csv(final_submission, ensemble_file, row.names = FALSE, quote = FALSE)

cat(paste("Ensemble submission file created successfully at:", ensemble_file, "\n"))
